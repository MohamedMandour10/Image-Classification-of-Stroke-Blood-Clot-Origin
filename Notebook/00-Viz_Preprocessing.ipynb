{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T15:22:24.357523Z",
     "iopub.status.busy": "2025-01-01T15:22:24.357252Z",
     "iopub.status.idle": "2025-01-01T15:22:24.367696Z",
     "shell.execute_reply": "2025-01-01T15:22:24.366849Z",
     "shell.execute_reply.started": "2025-01-01T15:22:24.357498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import keras.backend as K \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import openslide\n",
    "from openslide import OpenSlide\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "import gc\n",
    "import torch\n",
    "from torchvision.transforms import functional as TF\n",
    "import shutil\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T15:22:24.369193Z",
     "iopub.status.busy": "2025-01-01T15:22:24.368897Z",
     "iopub.status.idle": "2025-01-01T15:22:24.387844Z",
     "shell.execute_reply": "2025-01-01T15:22:24.386986Z",
     "shell.execute_reply.started": "2025-01-01T15:22:24.369169Z"
    },
    "papermill": {
     "duration": 0.031325,
     "end_time": "2022-12-13T12:03:24.419596",
     "exception": false,
     "start_time": "2022-12-13T12:03:24.388271",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>center_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>image_num</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>006388_0</td>\n",
       "      <td>11</td>\n",
       "      <td>006388</td>\n",
       "      <td>0</td>\n",
       "      <td>CE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>008e5c_0</td>\n",
       "      <td>11</td>\n",
       "      <td>008e5c</td>\n",
       "      <td>0</td>\n",
       "      <td>CE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00c058_0</td>\n",
       "      <td>11</td>\n",
       "      <td>00c058</td>\n",
       "      <td>0</td>\n",
       "      <td>LAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01adc5_0</td>\n",
       "      <td>11</td>\n",
       "      <td>01adc5</td>\n",
       "      <td>0</td>\n",
       "      <td>LAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>026c97_0</td>\n",
       "      <td>4</td>\n",
       "      <td>026c97</td>\n",
       "      <td>0</td>\n",
       "      <td>CE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id  center_id patient_id  image_num label\n",
       "0  006388_0         11     006388          0    CE\n",
       "1  008e5c_0         11     008e5c          0    CE\n",
       "2  00c058_0         11     00c058          0   LAA\n",
       "3  01adc5_0         11     01adc5          0   LAA\n",
       "4  026c97_0          4     026c97          0    CE"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../input/mayo-clinic-strip-ai/train.csv')\n",
    "test_df  = pd.read_csv('../input/mayo-clinic-strip-ai/test.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data viusalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T15:22:24.391962Z",
     "iopub.status.busy": "2025-01-01T15:22:24.391586Z",
     "iopub.status.idle": "2025-01-01T15:22:24.710633Z",
     "shell.execute_reply": "2025-01-01T15:22:24.709807Z",
     "shell.execute_reply.started": "2025-01-01T15:22:24.391926Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size = 754\n",
      "Test Size = 4\n"
     ]
    }
   ],
   "source": [
    "plt.style.use('Solarize_Light2')\n",
    "labels = train_df.groupby('label')['label'].count().div(len(train_df)).mul(100)\n",
    "centers = train_df.groupby(\"center_id\")['center_id'].count().div(len(train_df)).mul(100)\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,5))\n",
    "sns.barplot(x=labels.index, y=labels.values, ax=ax[0])\n",
    "ax[0].set_title(\"Distribution of a target variable\"), ax[0].set_ylabel(\"%\")\n",
    "sns.barplot(x=centers.index, y=centers.values, ax=ax[1])\n",
    "ax[1].set_title(\"Images per clinic center\"), ax[1].set_ylabel(\"%\")\n",
    "plt.show()\n",
    "print('Train Size = {}'.format(len(train_df)))\n",
    "print('Test Size = {}'.format(len(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From this plot we notice that there is a class imbalance which we will deal with later in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T15:22:25.297218Z",
     "iopub.status.busy": "2025-01-01T15:22:25.296444Z",
     "iopub.status.idle": "2025-01-01T15:22:25.308447Z",
     "shell.execute_reply": "2025-01-01T15:22:25.307334Z",
     "shell.execute_reply.started": "2025-01-01T15:22:25.297183Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in a training set: 754\n",
      "Number of images in a training set: 4\n",
      "Number of other: 396\n"
     ]
    }
   ],
   "source": [
    "train_images = glob(\"/kaggle/input/mayo-clinic-strip-ai/train/*\")\n",
    "test_images = glob(\"/kaggle/input/mayo-clinic-strip-ai/test/*\")\n",
    "other_images = glob(\"/kaggle/input/mayo-clinic-strip-ai/other/*\")\n",
    "print(f\"Number of images in a training set: {len(train_images)}\")\n",
    "print(f\"Number of images in a training set: {len(test_images)}\")\n",
    "print(f\"Number of other: {len(other_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img_prop = defaultdict(list)\n",
    "for i, path in enumerate(train_images):\n",
    "    img_path = train_images[i]\n",
    "    slide = OpenSlide(img_path)    \n",
    "    img_prop['image_id'].append(img_path[-12:-4])\n",
    "    img_prop['width'].append(slide.dimensions[0])\n",
    "    img_prop['height'].append(slide.dimensions[1])\n",
    "    img_prop['size'].append(round(os.path.getsize(img_path) / 1e6, 2))\n",
    "    img_prop['path'].append(img_path)\n",
    "image_data = pd.DataFrame(img_prop)\n",
    "image_data['img_aspect_ratio'] = image_data['width']/image_data['height']\n",
    "image_data.sort_values(by='image_id', inplace=True)\n",
    "image_data.reset_index(inplace=True, drop=True)\n",
    "image_data = image_data.merge(train_df, on='image_id')\n",
    "image_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('Solarize_Light2')\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,5))\n",
    "sns.histplot(x='size', data = image_data, bins=100, ax=ax[0])\n",
    "ax[0].set_title(\"Distribution of size\"), ax[0].set_ylabel(\"%\")\n",
    "sns.histplot(x='img_aspect_ratio', data = image_data, bins=100, ax=ax[1])\n",
    "ax[1].set_title(\"Image aspect ratio\"), ax[1].set_ylabel(\"%\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Some Image Displaying on the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def reading_tiff(image):\n",
    "    Reading_Image = cv2.cvtColor(cv2.imread(image),cv2.COLOR_BGR2RGB)\n",
    "    return Reading_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def single_display_tiff(image):\n",
    "    figure = plt.figure(figsize=(20,20))\n",
    "    Reading_Image = cv2.cvtColor(cv2.imread(image),cv2.COLOR_BGR2RGB)\n",
    "    plt.xlabel(Reading_Image.shape)\n",
    "    plt.ylabel(Reading_Image.size)\n",
    "    plt.title(\"TIFF\")\n",
    "    plt.imshow(Reading_Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def threshold_display(image):\n",
    "    figure = plt.figure(figsize=(8,8))\n",
    "    Reading_Image = cv2.cvtColor(cv2.imread(image),cv2.COLOR_BGR2RGB)\n",
    "    _,Threshold_Image = cv2.threshold(Reading_Image,30,255,cv2.THRESH_BINARY)\n",
    "    plt.xlabel(Threshold_Image.shape)\n",
    "    plt.ylabel(Threshold_Image.size)\n",
    "    plt.title(\"THRESHOLD\")\n",
    "    plt.imshow(Threshold_Image)\n",
    "    \n",
    "def threshold_reading(image):\n",
    "    Reading_Image = cv2.cvtColor(cv2.imread(image),cv2.COLOR_BGR2RGB)\n",
    "    _,Threshold_Image = cv2.threshold(Reading_Image,30,255,cv2.THRESH_BINARY)\n",
    "    return Threshold_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def adaptive_threshold_display(image):\n",
    "    figure = plt.figure(figsize=(8,8))\n",
    "    Reading_Image = cv2.imread(image,0)\n",
    "    Adaptive_Image = cv2.adaptiveThreshold(Reading_Image,20,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,9,2)\n",
    "    plt.xlabel(Adaptive_Image.shape)\n",
    "    plt.ylabel(Adaptive_Image.size)\n",
    "    plt.title(\"ADAPTIVE_THRESHOLD\")\n",
    "    plt.imshow(Adaptive_Image)\n",
    "    \n",
    "def adaptive_threshold_reading(image):\n",
    "    Reading_Image = cv2.imread(image,0)\n",
    "    Adaptive_Image = cv2.adaptiveThreshold(Reading_Image,20,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,9,2)\n",
    "    return Adaptive_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def canny_reading(image):\n",
    "    Reading_Image = cv2.cvtColor(cv2.imread(image),cv2.COLOR_BGR2RGB)\n",
    "    Canny_Image = cv2.Canny(Reading_Image,5,100)\n",
    "    return Canny_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def bitwise_and_display(image):\n",
    "    figure = plt.figure(figsize=(8,8))\n",
    "    Reading_Image = cv2.cvtColor(cv2.imread(image),cv2.COLOR_BGR2RGB)\n",
    "    Reading_Image = cv2.resize(Reading_Image,(180,180))\n",
    "    _,Threshold_Image = cv2.threshold(Reading_Image,30,255,cv2.THRESH_BINARY)\n",
    "    Threshold_Image = cv2.resize(Threshold_Image,(180,180))\n",
    "    Mask_For_Image = cv2.inRange(Reading_Image,Reading_Image,Threshold_Image)\n",
    "    Bitwise_Image = cv2.bitwise_and(Reading_Image,Reading_Image,mask=Mask_For_Image)\n",
    "    plt.xlabel(Bitwise_Image.shape)\n",
    "    plt.ylabel(Bitwise_Image.size)\n",
    "    plt.title(\"Bitwise_Image\")\n",
    "    plt.imshow(Bitwise_Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def different_type_display(image_one,image_two,image_three,image_four):\n",
    "    figure,axis = plt.subplots(1,4,figsize=(10,12))\n",
    "    axis[0].imshow(image_one)\n",
    "    axis[0].set_xlabel(image_one.shape)\n",
    "    axis[0].set_ylabel(image_one.size)\n",
    "    axis[0].set_title(\"image_one\")\n",
    "    axis[1].imshow(image_two)\n",
    "    axis[1].set_xlabel(image_two.shape)\n",
    "    axis[1].set_ylabel(image_two.size)\n",
    "    axis[1].set_title(\"image_two\")\n",
    "    axis[2].imshow(image_three)\n",
    "    axis[2].set_xlabel(image_three.shape)\n",
    "    axis[2].set_ylabel(image_three.size)\n",
    "    axis[2].set_title(\"image_three\")\n",
    "    axis[3].imshow(image_four)\n",
    "    axis[3].set_xlabel(image_four.shape)\n",
    "    axis[3].set_ylabel(image_four.size)\n",
    "    axis[3].set_title(\"image_four\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dispalying TIFF images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")\n",
    "single_display_tiff('../input/mayo-clinic-strip-ai/train/0cc0bc_0.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")\n",
    "threshold_display('../input/mayo-clinic-strip-ai/train/0cc0bc_0.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")\n",
    "adaptive_threshold_display('../input/mayo-clinic-strip-ai/train/0cc0bc_0.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8,8))\n",
    "Canny_Image = canny_reading('../input/mayo-clinic-strip-ai/train/0cc0bc_0.tif')\n",
    "plt.imshow(Canny_Image)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bitwise_and_display('../input/mayo-clinic-strip-ai/train/0cc0bc_0.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analyzing CE class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Reading_Image=reading_tiff('../input/mayo-clinic-strip-ai/train/31adaa_0.tif')\n",
    "figure = plt.figure(figsize=(20,20))\n",
    "plt.imshow(Reading_Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "different_type_display(reading_tiff('../input/mayo-clinic-strip-ai/train/31adaa_0.tif'),\n",
    "                      threshold_reading('../input/mayo-clinic-strip-ai/train/31adaa_0.tif'),\n",
    "                      adaptive_threshold_reading('../input/mayo-clinic-strip-ai/train/31adaa_0.tif'),\n",
    "                      canny_reading('../input/mayo-clinic-strip-ai/train/31adaa_0.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")\n",
    "single_display_tiff('../input/mayo-clinic-strip-ai/train/1b86c5_0.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "different_type_display(reading_tiff('../input/mayo-clinic-strip-ai/train/1b86c5_0.tif'),\n",
    "                      threshold_reading('../input/mayo-clinic-strip-ai/train/1b86c5_0.tif'),\n",
    "                      adaptive_threshold_reading('../input/mayo-clinic-strip-ai/train/1b86c5_0.tif'),\n",
    "                      canny_reading('../input/mayo-clinic-strip-ai/train/1b86c5_0.tif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analyzing LAA class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")\n",
    "single_display_tiff('../input/mayo-clinic-strip-ai/train/6f6e0c_0.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "different_type_display(reading_tiff('../input/mayo-clinic-strip-ai/train/6f6e0c_0.tif'),\n",
    "                      threshold_reading('../input/mayo-clinic-strip-ai/train/6f6e0c_0.tif'),\n",
    "                      adaptive_threshold_reading('../input/mayo-clinic-strip-ai/train/6f6e0c_0.tif'),\n",
    "                      canny_reading('../input/mayo-clinic-strip-ai/train/6f6e0c_0.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")\n",
    "single_display_tiff('../input/mayo-clinic-strip-ai/train/ed5006_0.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")\n",
    "single_display_tiff('../input/mayo-clinic-strip-ai/train/a2c497_0.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None \n",
    "\n",
    "CE_imgs = image_data.loc[image_data['label']=='CE','path']\n",
    "LAA_imgs = image_data.loc[image_data['label']=='LAA','path']\n",
    "\n",
    "\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(1,5, figsize=(16,16))\n",
    "train_images\n",
    "for ax in axes.reshape(-1):\n",
    "    img_path = np.random.choice(CE_imgs)\n",
    "    img = Image.open(img_path)   \n",
    "    img.thumbnail((300,300), Image.Resampling.LANCZOS)\n",
    "    ax.imshow(img), ax.set_title(\"target: CE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,5, figsize=(16,16))\n",
    "train_images\n",
    "for ax in axes.reshape(-1):\n",
    "    img_path = np.random.choice(LAA_imgs)\n",
    "    img = Image.open(img_path)   \n",
    "    img.thumbnail((300,300), Image.Resampling.LANCZOS)\n",
    "    ax.imshow(img), ax.set_title(\"target: LAA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "slide = OpenSlide('/kaggle/input/mayo-clinic-strip-ai/train/026c97_0.tif') # opening a full slide\n",
    "\n",
    "region = (2500, 2000) # location of the top left pixel\n",
    "level = 0 # level of the picture (we have only 0)\n",
    "size = (3500, 3500) # region size in pixels\n",
    "\n",
    "region = slide.read_region(region, level, size)\n",
    "image = region.resize((512, 512))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Solution 1:**\n",
    "Balancing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T15:22:31.859790Z",
     "iopub.status.busy": "2025-01-01T15:22:31.858834Z",
     "iopub.status.idle": "2025-01-01T15:22:31.867187Z",
     "shell.execute_reply": "2025-01-01T15:22:31.865974Z",
     "shell.execute_reply.started": "2025-01-01T15:22:31.859747Z"
    },
    "papermill": {
     "duration": 0.028968,
     "end_time": "2022-12-13T12:03:24.495672",
     "exception": false,
     "start_time": "2022-12-13T12:03:24.466704",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df[\"file_path\"] = train_df[\"image_id\"].apply(lambda x: \"../input/mayo-clinic-strip-ai/train/\" + x + \".tif\")\n",
    "test_df[\"file_path\"]  = test_df[\"image_id\"].apply(lambda x: \"../input/mayo-clinic-strip-ai/test/\" + x + \".tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T15:22:33.326368Z",
     "iopub.status.busy": "2025-01-01T15:22:33.325654Z",
     "iopub.status.idle": "2025-01-01T15:22:33.338489Z",
     "shell.execute_reply": "2025-01-01T15:22:33.337563Z",
     "shell.execute_reply.started": "2025-01-01T15:22:33.326334Z"
    },
    "papermill": {
     "duration": 0.017639,
     "end_time": "2022-12-13T12:03:24.519397",
     "exception": false,
     "start_time": "2022-12-13T12:03:24.501758",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>center_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>image_num</th>\n",
       "      <th>label</th>\n",
       "      <th>file_path</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>006388_0</td>\n",
       "      <td>11</td>\n",
       "      <td>006388</td>\n",
       "      <td>0</td>\n",
       "      <td>CE</td>\n",
       "      <td>../input/mayo-clinic-strip-ai/train/006388_0.tif</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>008e5c_0</td>\n",
       "      <td>11</td>\n",
       "      <td>008e5c</td>\n",
       "      <td>0</td>\n",
       "      <td>CE</td>\n",
       "      <td>../input/mayo-clinic-strip-ai/train/008e5c_0.tif</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00c058_0</td>\n",
       "      <td>11</td>\n",
       "      <td>00c058</td>\n",
       "      <td>0</td>\n",
       "      <td>LAA</td>\n",
       "      <td>../input/mayo-clinic-strip-ai/train/00c058_0.tif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01adc5_0</td>\n",
       "      <td>11</td>\n",
       "      <td>01adc5</td>\n",
       "      <td>0</td>\n",
       "      <td>LAA</td>\n",
       "      <td>../input/mayo-clinic-strip-ai/train/01adc5_0.tif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>026c97_0</td>\n",
       "      <td>4</td>\n",
       "      <td>026c97</td>\n",
       "      <td>0</td>\n",
       "      <td>CE</td>\n",
       "      <td>../input/mayo-clinic-strip-ai/train/026c97_0.tif</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id  center_id patient_id  image_num label  \\\n",
       "0  006388_0         11     006388          0    CE   \n",
       "1  008e5c_0         11     008e5c          0    CE   \n",
       "2  00c058_0         11     00c058          0   LAA   \n",
       "3  01adc5_0         11     01adc5          0   LAA   \n",
       "4  026c97_0          4     026c97          0    CE   \n",
       "\n",
       "                                          file_path  target  \n",
       "0  ../input/mayo-clinic-strip-ai/train/006388_0.tif       1  \n",
       "1  ../input/mayo-clinic-strip-ai/train/008e5c_0.tif       1  \n",
       "2  ../input/mayo-clinic-strip-ai/train/00c058_0.tif       0  \n",
       "3  ../input/mayo-clinic-strip-ai/train/01adc5_0.tif       0  \n",
       "4  ../input/mayo-clinic-strip-ai/train/026c97_0.tif       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"target\"] = train_df[\"label\"].apply(lambda x : 1 if x==\"CE\" else 0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Disable decompression bomb warnings\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "# Directory containing original images\n",
    "ORIGINAL_IMAGES_DIR = \"../input/mayo-clinic-strip-ai/train\"  # Adjust the path as needed\n",
    "\n",
    "# Initialize a list to store image dimensions\n",
    "image_dimensions = []\n",
    "\n",
    "# Iterate through images and collect dimensions\n",
    "for image_name in os.listdir(ORIGINAL_IMAGES_DIR):\n",
    "    image_path = os.path.join(ORIGINAL_IMAGES_DIR, image_name)\n",
    "    try:\n",
    "        with Image.open(image_path) as img:  # Open the image\n",
    "            width, height = img.size\n",
    "            image_dimensions.append({\"image_name\": image_name, \"width\": width, \"height\": height})\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_name}: {e}\")\n",
    "\n",
    "# Convert to a DataFrame for better visualization\n",
    "image_dimensions_df = pd.DataFrame(image_dimensions)\n",
    "\n",
    "# Save dimensions to CSV\n",
    "OUTPUT_CSV = \"original_image_dimensions.csv\"\n",
    "image_dimensions_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"Dimensions saved to {OUTPUT_CSV}\")\n",
    "print(image_dimensions_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the dimensions CSV\n",
    "IMAGE_DIMENSIONS_CSV = \"original_image_dimensions.csv\"  # Path to your dimensions CSV\n",
    "image_dimensions_df = pd.read_csv(IMAGE_DIMENSIONS_CSV)\n",
    "\n",
    "# Calculate pixel size for each image\n",
    "image_dimensions_df['pixel_size'] = image_dimensions_df['height'] * image_dimensions_df['width']\n",
    "\n",
    "# Display the first few rows\n",
    "print(image_dimensions_df.head())\n",
    "\n",
    "# Compute statistics for pixel size\n",
    "pixel_size_stats = image_dimensions_df['pixel_size'].describe()\n",
    "\n",
    "print(\"\\nPixel Size Statistics:\")\n",
    "print(pixel_size_stats)\n",
    "\n",
    "# Plot histograms for pixel size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(image_dimensions_df['height'], bins=50, color='blue', alpha=0.7)\n",
    "plt.title(\"Height Distribution\")\n",
    "plt.xlabel(\"Height (pixels)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(image_dimensions_df['width'], bins=50, color='green', alpha=0.7)\n",
    "plt.title(\"Width Distribution\")\n",
    "plt.xlabel(\"Width (pixels)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(image_dimensions_df['pixel_size'], bins=50, color='orange', alpha=0.7)\n",
    "plt.title(\"Pixel Size Distribution\")\n",
    "plt.xlabel(\"Pixel Size (pixels)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the updated DataFrame with pixel size\n",
    "UPDATED_CSV = \"updated_image_dimensions.csv\"\n",
    "image_dimensions_df.to_csv(UPDATED_CSV, index=False)\n",
    "print(f\"Updated CSV saved to {UPDATED_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the updated CSV file containing image dimensions\n",
    "file_path = \"/kaggle/working/updated_image_dimensions.csv\"\n",
    "image_stats = pd.read_csv(file_path)\n",
    "\n",
    "# Define the resizing factor\n",
    "resize_factor = 1 / 10\n",
    "\n",
    "# Calculate the total resized pixel count\n",
    "image_stats['resized_pixels'] = (image_stats['width'] * resize_factor) * (image_stats['height'] * resize_factor)\n",
    "total_resized_pixels = image_stats['resized_pixels'].sum()\n",
    "\n",
    "total_resized_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the path to the resized images directory\n",
    "RESIZED_IMAGES_DIR = \"/kaggle/input/resized-mayo\"\n",
    "\n",
    "# Initialize total pixel count\n",
    "total_pixels_resized = 0\n",
    "\n",
    "# Iterate over all images in the directory\n",
    "for image_name in tqdm(os.listdir(RESIZED_IMAGES_DIR), desc=\"Counting Pixels in Resized Images\"):\n",
    "    image_path = os.path.join(RESIZED_IMAGES_DIR, image_name)\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            width, height = img.size\n",
    "            total_pixels_resized += width * height\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_name}: {e}\")\n",
    "\n",
    "total_pixels_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Reload the CSV with image dimensions\n",
    "image_data = pd.read_csv(\"/kaggle/working/updated_image_dimensions.csv\")\n",
    "\n",
    "# Constants\n",
    "TARGET_TOTAL_PIXELS = 12_000_000_000  # Maximum allowed total pixels\n",
    "MAX_RESIZE_FACTOR = 1 / 11\n",
    "MIN_RESIZE_FACTOR = 1 / 7\n",
    "\n",
    "# Compute pixel size for each image\n",
    "image_data['original_pixels'] = image_data['width'] * image_data['height']\n",
    "\n",
    "# Get min and max pixel sizes\n",
    "min_pixels = image_data['original_pixels'].min()\n",
    "max_pixels = image_data['original_pixels'].max()\n",
    "\n",
    "# Apply the linear scaling model for resize factors\n",
    "image_data['resize_factor'] = MAX_RESIZE_FACTOR + (MIN_RESIZE_FACTOR - MAX_RESIZE_FACTOR) * (\n",
    "    (max_pixels - image_data['original_pixels']) / (max_pixels - min_pixels)\n",
    ")\n",
    "\n",
    "# Calculate updated resized pixel size for verification\n",
    "image_data['updated_pixels'] = image_data['original_pixels'] * image_data['resize_factor']**2\n",
    "\n",
    "# Select relevant columns for output\n",
    "output_data = image_data[['image_name', 'width', 'height', 'original_pixels', 'resize_factor', 'updated_pixels']]\n",
    "\n",
    "# Save the results to a new CSV\n",
    "output_csv_path = \"/kaggle/working/image_resizing_factors_dynamic.csv\"\n",
    "output_data.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Resizing factors saved to {output_csv_path}\")\n",
    "print(f\"Total pixels after resizing: {total_resized_pixels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    print(\"Keeping session alive...\")\n",
    "    time.sleep(300)  # Wait 5 minutes (300 seconds) before printing again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the updated image dimensions CSV\n",
    "image_stats_path = \"/kaggle/working/updated_image_dimensions.csv\"\n",
    "image_stats = pd.read_csv(image_stats_path)\n",
    "\n",
    "# Define target total pixels\n",
    "TARGET_TOTAL_PIXELS = 12_000_000_000  # 12 billion pixels\n",
    "\n",
    "# Calculate current total pixels\n",
    "current_total_pixels = image_stats['pixel_size'].sum()\n",
    "\n",
    "# Calculate dynamic resizing factors\n",
    "image_stats['dynamic_resize_factor'] = image_stats['pixel_size'].apply(\n",
    "    lambda x: min(1/5, max(1/10, (TARGET_TOTAL_PIXELS / current_total_pixels) ** 0.5))\n",
    ")\n",
    "\n",
    "# Calculate the total number of pixels after applying resizing factors\n",
    "image_stats['resized_pixels'] = image_stats['pixel_size'] * (image_stats['dynamic_resize_factor'] ** 2)\n",
    "total_resized_pixels = image_stats['resized_pixels'].sum()\n",
    "\n",
    "# Save the updated resizing factors CSV\n",
    "output_csv_path = \"/kaggle/working/image_resizing_factors.csv\"\n",
    "image_stats.to_csv(output_csv_path, index=False)\n",
    "\n",
    "output_csv_path, current_total_pixels, total_resized_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc  # Import garbage collector\n",
    "\n",
    "# Set maximum image size to avoid limits in Pillow\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "# Define resizing factor\n",
    "RESIZE_FACTOR = 1 / 10  # Resize to 10% of the original size\n",
    "\n",
    "# Create a directory to save resized images (optional)\n",
    "RESIZED_IMAGES_DIR = \"./resized_images/\"\n",
    "os.makedirs(RESIZED_IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "def resize_image(image_path, factor=RESIZE_FACTOR):\n",
    "    \"\"\"\n",
    "    Resizes an image while maintaining its aspect ratio.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        factor (float): Resizing factor (e.g., 0.1 for 10% of original size).\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: Resized image.\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path)  # Open the image\n",
    "    original_width, original_height = img.size  # Get original dimensions\n",
    "\n",
    "    # Calculate new dimensions\n",
    "    new_width = int(original_width * factor)\n",
    "    new_height = int(original_height * factor)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)  # Use high-quality resampling\n",
    "    img.close()  # Close the original image to free memory\n",
    "    return resized_img\n",
    "\n",
    "# Process all images in the dataset\n",
    "for image_path in tqdm(train_df['file_path'], desc=\"Resizing Images\"):\n",
    "    try:\n",
    "        # Resize the image\n",
    "        resized_img = resize_image(image_path)\n",
    "\n",
    "        # Save resized image to disk\n",
    "        image_name = os.path.basename(image_path)  # Get the image name\n",
    "        resized_img.save(os.path.join(RESIZED_IMAGES_DIR, image_name))  # Save resized image\n",
    "\n",
    "        # Explicitly delete resized image from memory and collect garbage\n",
    "        del resized_img\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "print(\"Resizing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define the path to your resized images\n",
    "RESIZED_IMAGES_PATH = \"/kaggle/input/resized-mayo/\"\n",
    "\n",
    "# Select 5 random image names from the directory\n",
    "random_images = random.sample(image_names, 5)\n",
    "\n",
    "# Loop through each random image and display it using single_display_tiff\n",
    "for img_name in random_images:\n",
    "    print(f\"Displaying image: {img_name}\")\n",
    "    single_display_tiff(os.path.join(RESIZED_IMAGES_PATH, img_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the resized images directory and the CSV file\n",
    "RESIZED_IMAGES_DIR = \"/kaggle/input/resized-mayo\"\n",
    "CSV_FILE_PATH = \"/kaggle/input/mayo-clinic-strip-ai/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "train_df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Add the `.tif` extension to the image_id column to match the resized image filenames\n",
    "train_df['image_name'] = train_df['image_id'] + \".tif\"\n",
    "\n",
    "# Filter only the images that exist in the resized images directory\n",
    "resized_image_names = os.listdir(RESIZED_IMAGES_DIR)\n",
    "resized_df = train_df[train_df['image_name'].isin(resized_image_names)].copy()\n",
    "\n",
    "# Add the full path to the resized images\n",
    "resized_df['image_path'] = resized_df['image_name'].apply(lambda x: os.path.join(RESIZED_IMAGES_DIR, x))\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(resized_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Print the number of rows in the new DataFrame\n",
    "print(f\"Number of rows in the new DataFrame: {resized_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# # Assuming RESIZED_IMAGES_DIR contains the resized images\n",
    "# image_dimensions = []\n",
    "\n",
    "# print(\"Processing images...\")\n",
    "\n",
    "# for idx, image_name in enumerate(os.listdir(RESIZED_IMAGES_DIR), start=1):\n",
    "#     print(idx)\n",
    "#     image_path = os.path.join(RESIZED_IMAGES_DIR, image_name)\n",
    "#     img = Image.open(image_path)\n",
    "#     width, height = img.width, img.height\n",
    "    \n",
    "#     # Print the number, image name, and dimensions\n",
    "#     print(f\"Processing {idx}: {image_name} - Dimensions: {width}x{height}\")\n",
    "    \n",
    "#     image_dimensions.append({\"image_name\": image_name, \"width\": width, \"height\": height})\n",
    "#     img.close()\n",
    "\n",
    "# # Convert to DataFrame for better visualization\n",
    "# import pandas as pd\n",
    "# dimensions_df = pd.DataFrame(image_dimensions)\n",
    "\n",
    "# # Print all dimensions\n",
    "# print(dimensions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Functions for Normalization\n",
    "# def prune_image_rows_cols(im, mask, thr=0.995):\n",
    "#     \"\"\"\n",
    "#     Removes rows and columns with mostly background pixels.\n",
    "#     \"\"\"\n",
    "#     for l in reversed(range(im.shape[1])):  # Process columns\n",
    "#         if (np.sum(mask[:, l]) / float(mask.shape[0])) > thr:\n",
    "#             im = np.delete(im, l, 1)\n",
    "#     for l in reversed(range(im.shape[0])):  # Process rows\n",
    "#         if (np.sum(mask[l, :]) / float(mask.shape[1])) > thr:\n",
    "#             im = np.delete(im, l, 0)\n",
    "#     return im\n",
    "\n",
    "# def mask_median(im, val=255):\n",
    "#     \"\"\"\n",
    "#     Creates a mask for background pixels and normalizes the image.\n",
    "#     \"\"\"\n",
    "#     masks = [None] * 3\n",
    "#     for c in range(3):  # Process each channel (R, G, B)\n",
    "#         masks[c] = im[..., c] >= np.median(im[:, :, c]) - 5\n",
    "#     mask = np.logical_and(*masks)  # Combine masks for all channels\n",
    "#     im[mask, :] = val  # Set background pixels to the specified value\n",
    "#     return im, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Dataset directory\n",
    "# OUTPUT_IMAGES_DIR = \"./normalized_images/\"  # Directory to save normalized images\n",
    "# os.makedirs(OUTPUT_IMAGES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def normalize_image(image_path, output_dir):\n",
    "#     \"\"\"\n",
    "#     Applies background removal and pruning to an image and saves the result.\n",
    "#     \"\"\"\n",
    "#     img = np.array(Image.open(image_path))  # Load image as a NumPy array\n",
    "#     img, mask = mask_median(img)  # Apply background mask\n",
    "#     img = prune_image_rows_cols(img, mask)  # Prune empty rows and columns\n",
    "#     output_path = os.path.join(output_dir, os.path.basename(image_path))\n",
    "#     Image.fromarray(img).save(output_path)  # Save normalized image\n",
    "\n",
    "# # Process all resized images\n",
    "# for idx, image_path in enumerate(tqdm(os.listdir(RESIZED_IMAGES_DIR), desc=\"Normalizing Images\")):\n",
    "#     try:\n",
    "#         image_full_path = os.path.join(RESIZED_IMAGES_DIR, image_path)\n",
    "#         normalize_image(image_full_path, OUTPUT_IMAGES_DIR)  # Normalize and save\n",
    "#         gc.collect()  # Free memory\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "# print(\"Normalization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# import os\n",
    "\n",
    "# # Function to display random images\n",
    "# def display_random_images(resized_dir, output_dir, num_images=5):\n",
    "#     \"\"\"\n",
    "#     Display random images from the resized directory and their corresponding normalized images.\n",
    "    \n",
    "#     Args:\n",
    "#         resized_dir (str): Directory containing resized images.\n",
    "#         output_dir (str): Directory containing normalized images.\n",
    "#         num_images (int): Number of random images to display.\n",
    "#     \"\"\"\n",
    "#     # Get the list of image names\n",
    "#     resized_images = os.listdir(resized_dir)\n",
    "    \n",
    "#     # Randomly select images\n",
    "#     random_images = random.sample(resized_images, num_images)\n",
    "    \n",
    "#     for img_name in random_images:\n",
    "#         print(f\"Displaying image: {img_name}\")\n",
    "        \n",
    "#         # Get the paths for the resized and normalized images\n",
    "#         resized_image_path = os.path.join(resized_dir, img_name)\n",
    "#         normalized_image_path = os.path.join(output_dir, img_name)\n",
    "        \n",
    "#         # Display the resized image\n",
    "#         print(\"Original Resized Image:\")\n",
    "#         single_display_tiff(resized_image_path)\n",
    "        \n",
    "#         # Display the corresponding normalized image\n",
    "#         print(\"Normalized Image:\")\n",
    "#         single_display_tiff(normalized_image_path)\n",
    "\n",
    "# # Call the function\n",
    "# display_random_images(RESIZED_IMAGES_DIR, OUTPUT_IMAGES_DIR, num_images=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T15:44:52.061788Z",
     "iopub.status.busy": "2025-01-01T15:44:52.061341Z",
     "iopub.status.idle": "2025-01-01T15:44:52.152151Z",
     "shell.execute_reply": "2025-01-01T15:44:52.151175Z",
     "shell.execute_reply.started": "2025-01-01T15:44:52.061750Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your Hugging Face token here\n",
    "login(token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T15:53:19.203460Z",
     "iopub.status.busy": "2025-01-01T15:53:19.202525Z",
     "iopub.status.idle": "2025-01-01T15:53:19.207729Z",
     "shell.execute_reply": "2025-01-01T15:53:19.206745Z",
     "shell.execute_reply.started": "2025-01-01T15:53:19.203422Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import login, list_repo_files, delete_file\n",
    "\n",
    "# # Specify your repository\n",
    "# repo_id = \"AlyMaged/patches300\"  # Replace with your repository name\n",
    "# repo_type = \"dataset\"  # Replace with the repository type (e.g., \"dataset\" or \"model\")\n",
    "\n",
    "# # List all files in the repository\n",
    "# files = list_repo_files(repo_id=repo_id, repo_type=repo_type)\n",
    "\n",
    "# # Loop through files and delete all except .gitattributes\n",
    "# for file in files:\n",
    "#     if file != \".gitattributes\":\n",
    "#         try:\n",
    "#             delete_file(\n",
    "#                 path_in_repo=file,\n",
    "#                 repo_id=repo_id,\n",
    "#                 repo_type=repo_type,\n",
    "#                 commit_message=f\"Deleted {file} (except .gitattributes)\"\n",
    "#             )\n",
    "#             print(f\"Deleted: {file}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to delete {file}: {e}\")\n",
    "\n",
    "# print(\"All files except .gitattributes have been deleted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T16:10:54.481181Z",
     "iopub.status.busy": "2025-01-01T16:10:54.480828Z",
     "iopub.status.idle": "2025-01-01T16:12:29.179282Z",
     "shell.execute_reply": "2025-01-01T16:12:29.178272Z",
     "shell.execute_reply.started": "2025-01-01T16:10:54.481154Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created zip file: patches300.zip\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, upload_folder, upload_file\n",
    "\n",
    "# Step 2: Define paths and repository details\n",
    "local_folder_path = \"./patches_output\"  # Folder to be zipped\n",
    "zip_file_name = \"patches300.zip\"  # Name of the zip file\n",
    "csv_file_path = \"./patches_labels.csv\"  # Path to the CSV file\n",
    "repo_id = \"AlyMaged/patches300-3\"  # Replace with your Hugging Face repository name\n",
    "repo_type = \"dataset\"  # Specify the type as \"dataset\"\n",
    "\n",
    "# Step 3: Create a zip file for the patches directory\n",
    "shutil.make_archive(base_name=\"patches300\", format=\"zip\", root_dir=local_folder_path)\n",
    "\n",
    "print(f\"Created zip file: {zip_file_name}\")\n",
    "\n",
    "# # Step 4: Upload the zip file to Hugging Face\n",
    "# upload_file(\n",
    "#     path_or_fileobj=zip_file_name,\n",
    "#     path_in_repo=zip_file_name,\n",
    "#     repo_id=repo_id,\n",
    "#     repo_type=repo_type,\n",
    "#     commit_message=\"Uploaded zipped patches dataset\"\n",
    "# )\n",
    "\n",
    "# print(f\"Zip file '{zip_file_name}' uploaded successfully!\")\n",
    "\n",
    "# # Step 5: Upload the CSV file to Hugging Face\n",
    "# csv_file_name = \"patches_labels.csv\"  # The name of the CSV file in the repository\n",
    "# upload_file(\n",
    "#     path_or_fileobj=csv_file_path,\n",
    "#     path_in_repo=csv_file_name,\n",
    "#     repo_id=repo_id,\n",
    "#     repo_type=repo_type,\n",
    "#     commit_message=\"Uploaded patches metadata CSV\"\n",
    "# )\n",
    "\n",
    "# print(f\"CSV file '{csv_file_name}' uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Step 4: Upload the zip file to Hugging Face\n",
    "# upload_file(\n",
    "#     path_or_fileobj=zip_file_name,\n",
    "#     path_in_repo=zip_file_name,\n",
    "#     repo_id=repo_id,\n",
    "#     repo_type=repo_type,\n",
    "#     commit_message=\"Uploaded zipped patches dataset\"\n",
    "# )\n",
    "\n",
    "# print(f\"Zip file '{zip_file_name}' uploaded successfully!\")\n",
    "\n",
    "# # Step 5: Upload the CSV file to Hugging Face\n",
    "# csv_file_name = \"patches_labels.csv\"  # The name of the CSV file in the repository\n",
    "# upload_file(\n",
    "#     path_or_fileobj=csv_file_path,\n",
    "#     path_in_repo=csv_file_name,\n",
    "#     repo_id=repo_id,\n",
    "#     repo_type=repo_type,\n",
    "#     commit_message=\"Uploaded patches metadata CSV\"\n",
    "# )\n",
    "\n",
    "# print(f\"CSV file '{csv_file_name}' uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T16:12:40.154361Z",
     "iopub.status.busy": "2025-01-01T16:12:40.153460Z",
     "iopub.status.idle": "2025-01-01T16:12:40.159842Z",
     "shell.execute_reply": "2025-01-01T16:12:40.158794Z",
     "shell.execute_reply.started": "2025-01-01T16:12:40.154318Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Change to working directory\n",
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T16:12:46.855169Z",
     "iopub.status.busy": "2025-01-01T16:12:46.854450Z",
     "iopub.status.idle": "2025-01-01T16:12:46.861060Z",
     "shell.execute_reply": "2025-01-01T16:12:46.860108Z",
     "shell.execute_reply.started": "2025-01-01T16:12:46.855135Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='patches300.zip' target='_blank'>patches300.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/patches300.zip"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Generate the download link\n",
    "from IPython.display import FileLink\n",
    "FileLink('patches300.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T16:12:56.997901Z",
     "iopub.status.busy": "2025-01-01T16:12:56.997524Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping session alive...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    print(\"Keeping session alive...\")\n",
    "    time.sleep(300)  # Wait 5 minutes (300 seconds) before printing again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Path to your resized dataset\n",
    "# RESIZED_IMAGES_PATH = \"/kaggle/input/resized-mayo/\"\n",
    "\n",
    "# # List all image names in the directory\n",
    "# image_names = os.listdir(RESIZED_IMAGES_PATH)\n",
    "\n",
    "# # Print the names\n",
    "# print(\"Names of images in the dataset:\")\n",
    "# for image_name in image_names:\n",
    "#     print(image_name)\n",
    "\n",
    "# # Optionally, count the total number of images\n",
    "# print(f\"\\nTotal images: {len(image_names)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define directories\n",
    "RESIZED_IMAGES_DIR = \"/kaggle/input/resized-mayo/\"  # Adjust to your dataset path\n",
    "PATCHES_OUTPUT_DIR = \"./patches_output/\"\n",
    "os.makedirs(PATCHES_OUTPUT_DIR, exist_ok=True)  # Create output directory for patches\n",
    "\n",
    "# Define patch extraction parameters\n",
    "PATCH_SIZE = (300, 300)  # Height, Width of each patch\n",
    "STEP_SIZE = (300, 300)  # Step size for sliding window\n",
    "\n",
    "# Read the train DataFrame with labels\n",
    "CSV_FILE_PATH = \"/kaggle/input/mayo-clinic-strip-ai/train.csv\"\n",
    "train_df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Add the `.tif` extension to match the resized images\n",
    "train_df['image_name'] = train_df['image_id'] + \".tif\"\n",
    "# Add the 'target' column based on the 'label' column\n",
    "train_df[\"target\"] = train_df[\"label\"].apply(lambda x: 1 if x == \"CE\" else 0)\n",
    "\n",
    "# Filter the resized images that exist in the directory\n",
    "resized_image_names = os.listdir(RESIZED_IMAGES_DIR)\n",
    "resized_df = train_df[train_df['image_name'].isin(resized_image_names)].copy()\n",
    "\n",
    "# Add the full path to the resized images\n",
    "resized_df['image_path'] = resized_df['image_name'].apply(lambda x: os.path.join(RESIZED_IMAGES_DIR, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to extract patches\n",
    "def extract_patches(image_path, patch_size, step_size, output_dir):\n",
    "    \"\"\"\n",
    "    Extracts patches from an image using a sliding window approach.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        patch_size (tuple): (height, width) of each patch.\n",
    "        step_size (tuple): Step size (vertical, horizontal) for sliding window.\n",
    "        output_dir (str): Directory to save patches.\n",
    "\n",
    "    Returns:\n",
    "        List of extracted patch filenames.\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path)  # Open the image\n",
    "    img = np.array(img)  # Convert to NumPy array\n",
    "\n",
    "    patches = []\n",
    "    patch_height, patch_width = patch_size\n",
    "    step_height, step_width = step_size\n",
    "    img_height, img_width = img.shape[:2]  # Get the image dimensions\n",
    "\n",
    "    patch_idx = 0  # Counter for patches\n",
    "    for y in range(0, img_height, step_height):\n",
    "        for x in range(0, img_width, step_width):\n",
    "            # Extract patch\n",
    "            patch = img[y:y + patch_height, x:x + patch_width]\n",
    "\n",
    "            # Skip incomplete patches at the edges\n",
    "            if patch.shape[0] != patch_height or patch.shape[1] != patch_width:\n",
    "                continue\n",
    "\n",
    "            # Save the patch\n",
    "            patch_name = f\"{os.path.basename(image_path).split('.')[0]}_patch_{patch_idx}.png\"\n",
    "            patch_path = os.path.join(output_dir, patch_name)\n",
    "            Image.fromarray(patch).save(patch_path)\n",
    "            patches.append(patch_name)\n",
    "            patch_idx += 1\n",
    "\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extract patches for all images in the resized dataset\n",
    "patches_data = []\n",
    "\n",
    "for idx, row in tqdm(resized_df.iterrows(), total=resized_df.shape[0], desc=\"Extracting Patches\"):\n",
    "    image_path = row['image_path']\n",
    "    label = row['label']\n",
    "    target = row['target']\n",
    "    try:\n",
    "        # Extract patches and save them\n",
    "        patch_files = extract_patches(image_path, PATCH_SIZE, STEP_SIZE, PATCHES_OUTPUT_DIR)\n",
    "\n",
    "        # Map patches to their labels\n",
    "        for patch_file in patch_files:\n",
    "            patches_data.append({\n",
    "                'patch_name': patch_file,\n",
    "                'original_image': os.path.basename(image_path),\n",
    "                'label': label,\n",
    "                'target': target\n",
    "            })\n",
    "\n",
    "        # Garbage collection to free memory\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save patch metadata to a DataFrame\n",
    "patches_df = pd.DataFrame(patches_data)\n",
    "\n",
    "# Save the patches metadata for later use\n",
    "patches_df.to_csv(\"patches_labels.csv\", index=False)\n",
    "\n",
    "# Display a sample of the patches DataFrame\n",
    "print(f\"Total patches: {len(patches_df)}\")\n",
    "print(patches_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T15:26:58.992118Z",
     "iopub.status.busy": "2025-01-01T15:26:58.991065Z",
     "iopub.status.idle": "2025-01-01T15:27:07.398705Z",
     "shell.execute_reply": "2025-01-01T15:27:07.397690Z",
     "shell.execute_reply.started": "2025-01-01T15:26:58.992067Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.64.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.1.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (2022.8.2)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (3.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface_hub) (3.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (2022.9.24)\n",
      "Installing collected packages: huggingface_hub\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cached-path 1.1.6 requires huggingface-hub<0.11.0,>=0.8.1, but you have huggingface-hub 0.16.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface_hub-0.16.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T15:27:07.619300Z",
     "iopub.status.busy": "2025-01-01T15:27:07.618574Z",
     "iopub.status.idle": "2025-01-01T15:27:07.704961Z",
     "shell.execute_reply": "2025-01-01T15:27:07.703958Z",
     "shell.execute_reply.started": "2025-01-01T15:27:07.619259Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Log in with your Hugging Face token\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T15:27:15.219900Z",
     "iopub.status.busy": "2025-01-01T15:27:15.219234Z",
     "iopub.status.idle": "2025-01-01T15:27:15.224693Z",
     "shell.execute_reply": "2025-01-01T15:27:15.223847Z",
     "shell.execute_reply.started": "2025-01-01T15:27:15.219864Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16.4\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "print(huggingface_hub.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    print(\"Keeping session alive...\")\n",
    "    time.sleep(300)  # Wait 5 minutes (300 seconds) before printing again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "PATCHES_DIR = \"./patches_output\"  # Directory containing patches\n",
    "PATCHES_LABELS_CSV = \"./patches_labels.csv\"  # CSV file with patch labels\n",
    "UPDATED_CSV = \"./updated_patches_labels.csv\"  # Output CSV file\n",
    "\n",
    "# Threshold for keeping patches\n",
    "CONTENT_THRESHOLD = 0.3  # 30%\n",
    "\n",
    "# Load patches_labels.csv\n",
    "patches_labels = pd.read_csv(PATCHES_LABELS_CSV)\n",
    "\n",
    "# Function to apply Otsu's thresholding and calculate content area percentage\n",
    "def is_patch_kept(patch_path):\n",
    "    image = cv2.imread(patch_path, cv2.IMREAD_GRAYSCALE)  # Load patch as grayscale\n",
    "    _, otsu_thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)  # Otsu's thresholding\n",
    "    content_area = cv2.countNonZero(otsu_thresh)  # Count non-zero (content) pixels\n",
    "    total_area = image.shape[0] * image.shape[1]  # Total number of pixels in the patch\n",
    "    content_percentage = content_area / total_area\n",
    "    return int(content_percentage >= CONTENT_THRESHOLD)  # Return 1 if kept, 0 otherwise\n",
    "\n",
    "# Add a 'kept' column to patches_labels\n",
    "patches_labels['kept'] = 0  # Initialize with 0\n",
    "for idx, row in tqdm(patches_labels.iterrows(), total=patches_labels.shape[0], desc=\"Processing Patches\"):\n",
    "    patch_name = row['patch_name']\n",
    "    patch_path = os.path.join(PATCHES_DIR, patch_name)\n",
    "    if os.path.exists(patch_path):\n",
    "        patches_labels.loc[idx, 'kept'] = is_patch_kept(patch_path)\n",
    "\n",
    "# Save the updated CSV\n",
    "patches_labels.to_csv(UPDATED_CSV, index=False)\n",
    "print(f\"Updated patches_labels.csv saved to {UPDATED_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import ImageOps  # Import ImageOps for adding borders\n",
    "\n",
    "# Load the updated patches_labels.csv\n",
    "UPDATED_CSV = \"./updated_patches_labels.csv\"  # Path to the updated CSV\n",
    "PATCHES_DIR = \"./patches_output\"  # Directory containing patches\n",
    "patches_labels = pd.read_csv(UPDATED_CSV)\n",
    "\n",
    "# Filter kept and removed patches\n",
    "kept_patches = patches_labels[patches_labels['kept'] == 1]\n",
    "removed_patches = patches_labels[patches_labels['kept'] == 0]\n",
    "\n",
    "# Randomly sample 100 patches from each group\n",
    "kept_samples = kept_patches.sample(n=min(500, len(kept_patches)), random_state=42)\n",
    "removed_samples = removed_patches.sample(n=min(500, len(removed_patches)), random_state=42)\n",
    "\n",
    "# Function to display patches in a grid with borders\n",
    "def display_patches_with_borders(samples, title, border_color=\"red\", border_width=5):\n",
    "    n_cols = 10\n",
    "    n_rows = (len(samples) + n_cols - 1) // n_cols  # Ensure enough rows\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 1.5))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (_, row) in enumerate(samples.iterrows()):\n",
    "        patch_path = os.path.join(PATCHES_DIR, row['patch_name'])\n",
    "        if os.path.exists(patch_path):\n",
    "            img = Image.open(patch_path)\n",
    "            # Add a border to the image\n",
    "            img_with_border = ImageOps.expand(img, border=border_width, fill=border_color)\n",
    "            axes[i].imshow(img_with_border)\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[i].axis('off')  # Leave the subplot blank if the file doesn't exist\n",
    "\n",
    "    # Hide unused axes\n",
    "    for j in range(len(samples), len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)  # Adjust title position\n",
    "    plt.show()\n",
    "\n",
    "# Display kept patches with green borders\n",
    "display_patches_with_borders(kept_samples, \"Kept Patches (100 Random Examples)\", border_color=\"green\", border_width=5)\n",
    "\n",
    "# Display removed patches with red borders\n",
    "display_patches_with_borders(removed_samples, \"Removed Patches (100 Random Examples)\", border_color=\"red\", border_width=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the directory and output zip file name\n",
    "PATCHES_DIR = \"./patches_output\"\n",
    "ZIP_FILE_NAME = \"patches_output.zip\"\n",
    "\n",
    "# Zip the directory\n",
    "shutil.make_archive(PATCHES_DIR, 'zip', PATCHES_DIR)\n",
    "print(f\"{ZIP_FILE_NAME} has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Change to working directory\n",
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Display download links\n",
    "print(\"Download Links:\")\n",
    "print(\"Patches Output ZIP:\")\n",
    "display(FileLink(\"patches_output.zip\"))\n",
    "\n",
    "print(\"Updated Patches Labels CSV:\")\n",
    "display(FileLink(\"updated_patches_labels.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2496.090688,
     "end_time": "2022-12-13T12:45:00.650948",
     "exception": false,
     "start_time": "2022-12-13T12:03:24.56026",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# def preprocess(image_path):\n",
    "#     slide=OpenSlide(image_path)\n",
    "#     region= (1000,1000)    \n",
    "#     size  = (5000, 5000)\n",
    "#     image = slide.read_region(region, 0, size)\n",
    "#     image = image.resize((512, 512))\n",
    "#     image = np.array(image)    \n",
    "#     return image\n",
    "\n",
    "# X_train=[]\n",
    "# for i in tqdm(train_df['file_path']):\n",
    "#     x1=preprocess(i)\n",
    "#     X_train.append(x1)\n",
    "\n",
    "# Y_train=[]    \n",
    "# Y_train=train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# PATCH_SIZE = (600, 600)  \n",
    "# STEP_SIZE = (600, 600)  \n",
    "# def preprocess(image_path):\n",
    "    \n",
    "#     slide = OpenSlide(image_path) \n",
    "#     width, height = slide.dimensions  \n",
    "#     patches = []  \n",
    "\n",
    "#     for y in range(0, height, STEP_SIZE[1]): \n",
    "#         for x in range(0, width, STEP_SIZE[0]):\n",
    "#             patch_width = min(PATCH_SIZE[0], width - x)\n",
    "#             patch_height = min(PATCH_SIZE[1], height - y)\n",
    "#             if patch_width < PATCH_SIZE[0] or patch_height < PATCH_SIZE[1]:\n",
    "#                 continue\n",
    "            \n",
    "#             patch = slide.read_region((x, y), 0, PATCH_SIZE)\n",
    "#             patch = np.array(patch)  \n",
    "#             patches.append(patch)\n",
    "            \n",
    "#     slide.close()\n",
    "#     return patches\n",
    "\n",
    "\n",
    "# X_train = []  \n",
    "# Y_train = []  \n",
    "    \n",
    "# for i in tqdm(train_df['file_path']):\n",
    "#     patches = preprocess(i) \n",
    "#     X_train.extend(patches)  \n",
    "#     Y_train.extend([train_df['target'][train_df['file_path'] == i].iloc[0]] * len(patches))  # Assign labels\n",
    "\n",
    "#     # Use garbage collection\n",
    "#     gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_background(image_path, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Remove the background from the whole slide image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the WSI file.\n",
    "        threshold (float): Threshold for background detection (default=0.9).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Background-removed image as a NumPy array.\n",
    "    \"\"\"\n",
    "    slide = OpenSlide(image_path)\n",
    "    image = slide.read_region((0, 0), 0, slide.dimensions)  # Load the whole slide at level 0\n",
    "    image = np.array(image)  # Convert to NumPy array\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Thresholding to create a mask for non-background pixels\n",
    "    _, mask = cv2.threshold(gray_image, int(threshold * 255), 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Apply the mask to the original image\n",
    "    mask = mask.astype(bool)  # Convert mask to boolean\n",
    "    background_removed_image = np.zeros_like(image)\n",
    "    for c in range(3):  # Apply mask for each channel (R, G, B)\n",
    "        background_removed_image[..., c] = image[..., c] * mask\n",
    "    \n",
    "    slide.close()\n",
    "    return background_removed_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Visualize a background-removed image\n",
    "# example_path = train_df['file_path'][0]\n",
    "# cleaned_image = remove_background(example_path)\n",
    "# plt.imshow(cleaned_image)\n",
    "# plt.title(\"Background-Removed Image\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess(cleaned_image):\n",
    "    \"\"\"\n",
    "    Divide the background-removed image into patches.\n",
    "\n",
    "    Args:\n",
    "        cleaned_image (numpy.ndarray): Background-removed WSI.\n",
    "\n",
    "    Returns:\n",
    "        list: List of extracted patches.\n",
    "    \"\"\"\n",
    "    height, width, _ = cleaned_image.shape\n",
    "    patches = []  # Store extracted patches\n",
    "\n",
    "    # Loop through the image using a sliding window\n",
    "    for y in range(0, height, STEP_SIZE[1]): \n",
    "        for x in range(0, width, STEP_SIZE[0]):\n",
    "            # Ensure patch dimensions do not exceed image boundaries\n",
    "            patch_width = min(PATCH_SIZE[0], width - x)\n",
    "            patch_height = min(PATCH_SIZE[1], height - y)\n",
    "            if patch_width < PATCH_SIZE[0] or patch_height < PATCH_SIZE[1]:\n",
    "                continue\n",
    "            \n",
    "            # Extract the patch\n",
    "            patch = cleaned_image[y:y + patch_height, x:x + patch_width, :]\n",
    "            patch = patch / 255.0  # Normalize pixel values\n",
    "            patches.append(patch)\n",
    "    \n",
    "    return patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def is_low_quality(patch, threshold=0.3):\n",
    "\n",
    "    gray_patch = cv2.cvtColor(patch, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    _, binary_patch = cv2.threshold(gray_patch, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    tissue_pixels = np.sum(binary_patch == 0) \n",
    "    total_pixels = binary_patch.size\n",
    "    tissue_ratio = tissue_pixels / total_pixels\n",
    "\n",
    "    return tissue_ratio < threshold\n",
    "\n",
    "def filter_low_quality_patches(X_train, Y_train, threshold=0.3):\n",
    "\n",
    "    filtered_X_train = []\n",
    "    filtered_Y_train = []\n",
    "    \n",
    "    for patch, label in tqdm(zip(X_train, Y_train), total=len(X_train), desc=\"Filtering low-quality patches\"):\n",
    "        if not is_low_quality(patch, threshold):\n",
    "            filtered_X_train.append(patch)\n",
    "            filtered_Y_train.append(label)\n",
    "    \n",
    "    return filtered_X_train, filtered_Y_train\n",
    "\n",
    "filtered_X_train, filtered_Y_train = filter_low_quality_patches(X_train, Y_train, threshold=0.3)\n",
    "\n",
    "print(f\"Original number of patches: {len(X_train)}\")\n",
    "print(f\"Number of high-quality patches: {len(filtered_X_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.core.composition import OneOf\n",
    "from albumentations.augmentations.transforms import HorizontalFlip, VerticalFlip, RandomRotate90, RandomBrightnessContrast\n",
    "from albumentations.augmentations.geometric.transforms import ShiftScaleRotate\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "augmentation_pipeline = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),             \n",
    "    A.VerticalFlip(p=0.5),              \n",
    "    A.RandomRotate90(p=0.5),              \n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5)\n",
    "])\n",
    "\n",
    "def augment_patch(patch):\n",
    "    \n",
    "    augmented = augmentation_pipeline(image=patch)\n",
    "    return augmented['image']\n",
    "\n",
    "def apply_augmentation(X_train, Y_train, augmentation_ratio=0.5):\n",
    "\n",
    "    augmented_X = []\n",
    "    augmented_Y = []\n",
    "\n",
    "    num_to_augment = int(len(X_train) * augmentation_ratio)  \n",
    "\n",
    "    for i in tqdm(range(num_to_augment), desc=\"Applying augmentations\"):\n",
    "        # Select a random patch and its label\n",
    "        idx = np.random.randint(0, len(X_train))\n",
    "        patch = X_train[idx]\n",
    "        label = Y_train[idx]\n",
    "\n",
    "        augmented_patch = augment_patch(patch)\n",
    "\n",
    "        augmented_X.append(augmented_patch)\n",
    "        augmented_Y.append(label)\n",
    "\n",
    "    final_X_train = X_train + augmented_X\n",
    "    final_Y_train = Y_train + augmented_Y\n",
    "\n",
    "    return final_X_train, final_Y_train\n",
    "\n",
    "augmented_X_train, augmented_Y_train = apply_augmentation(filtered_X_train, filtered_Y_train, augmentation_ratio=0.5)\n",
    "\n",
    "print(f\"Original dataset size: {len(filtered_X_train)}\")\n",
    "print(f\"Augmented dataset size: {len(augmented_X_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def resize_patch(patch, target_size=(256, 256)):\n",
    "\n",
    "    resized_patch = cv2.resize(patch, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "    return resized_patch\n",
    "\n",
    "def resize_dataset(X_train, target_size=(256, 256)):\n",
    "\n",
    "    resized_X_train = []\n",
    "    for patch in tqdm(X_train, desc=\"Resizing patches\"):\n",
    "        resized_patch = resize_patch(patch, target_size=target_size)\n",
    "        resized_X_train.append(resized_patch)\n",
    "    return resized_X_train\n",
    "\n",
    "resized_X_train = resize_dataset(augmented_X_train, target_size=(256, 256))\n",
    "\n",
    "print(f\"Original patch size: {augmented_X_train[0].shape}\")\n",
    "print(f\"Resized patch size: {resized_X_train[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_imagenet(X_train):\n",
    "\n",
    "    imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "    imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    X_train /= 255.0  # Scale to [0, 1]\n",
    "    \n",
    "    X_train -= imagenet_mean\n",
    "    X_train /= imagenet_std\n",
    "    return X_train\n",
    "\n",
    "imagenet_normalized_X_train = normalize_imagenet(resized_X_train)\n",
    "\n",
    "print(f\"Pixel range after ImageNet normalization: {imagenet_normalized_X_train[0].min()} to {imagenet_normalized_X_train[0].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.296197,
     "end_time": "2022-12-13T12:45:01.078179",
     "exception": false,
     "start_time": "2022-12-13T12:45:00.781982",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train=np.array(X_train)\n",
    "X_train=X_train/255.0\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.049801,
     "end_time": "2022-12-13T12:45:01.170206",
     "exception": false,
     "start_time": "2022-12-13T12:45:01.120405",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Splitting data\n",
    "x_train,x_test,y_train,y_test=train_test_split(X_train,Y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.050654,
     "end_time": "2022-12-13T12:45:01.261913",
     "exception": false,
     "start_time": "2022-12-13T12:45:01.211259",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 3949526,
     "sourceId": 37333,
     "sourceType": "competition"
    },
    {
     "datasetId": 6368812,
     "sourceId": 10290694,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30302,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
